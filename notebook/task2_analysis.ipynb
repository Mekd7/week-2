{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19ccff08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank\n",
      "BOA       408\n",
      "CBE       403\n",
      "Dashen    375\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfd5e889dd846598a32d2bab0bd82ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mekde\\Desktop\\week-2\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mekde\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87267ac5f2274a10bd1f257913d568b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae7dd41d5814147bb6918b1dc3ebd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b63b3e0b94a4d4eb5fd3df79d432cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved task2_with_sentiment.csv\n",
      "               count      mean\n",
      "bank   rating                 \n",
      "CBE    5         252  0.480916\n",
      "Dashen 5         229  0.510104\n",
      "BOA    1         175 -0.869354\n",
      "       5         172  0.366170\n",
      "CBE    1          70 -0.681755\n",
      "Dashen 1          59 -0.774107\n",
      "CBE    4          37  0.022754\n",
      "Dashen 4          35  0.179971\n",
      "       3          34 -0.003815\n",
      "BOA    3          29 -0.444899\n",
      "CBE    3          23 -0.474668\n",
      "       2          21 -0.612910\n",
      "BOA    4          19 -0.362784\n",
      "Dashen 2          18 -0.628444\n",
      "BOA    2          13 -0.945256\n",
      "\n",
      "Top keywords for BOA:\n",
      "  app (36.25)\n",
      "  good (19.40)\n",
      "  bank (12.43)\n",
      "  boa (10.83)\n",
      "  working (9.71)\n",
      "  work (9.60)\n",
      "  best (9.13)\n",
      "  mobile (8.32)\n",
      "  worst (8.31)\n",
      "  use (8.04)\n",
      "  great (7.88)\n",
      "  banking (7.55)\n",
      "  doesn (7.49)\n",
      "  like (6.85)\n",
      "  bad (6.61)\n",
      "\n",
      "Top keywords for CBE:\n",
      "  app (33.92)\n",
      "  good (29.72)\n",
      "  best (17.02)\n",
      "  cbe (12.45)\n",
      "  bank (11.00)\n",
      "  nice (10.12)\n",
      "  like (8.38)\n",
      "  application (7.29)\n",
      "  good app (7.17)\n",
      "  update (6.70)\n",
      "  excellent (6.19)\n",
      "  banking (5.16)\n",
      "  use (4.91)\n",
      "  best app (4.86)\n",
      "  apps (4.80)\n",
      "\n",
      "Top keywords for Dashen:\n",
      "  app (39.51)\n",
      "  good (25.65)\n",
      "  best (15.22)\n",
      "  bank (13.26)\n",
      "  use (9.89)\n",
      "  good app (9.54)\n",
      "  nice (9.32)\n",
      "  best app (9.23)\n",
      "  dashen (8.34)\n",
      "  amole (7.88)\n",
      "  easy (7.41)\n",
      "  dashen bank (7.23)\n",
      "  update (7.10)\n",
      "  like (6.77)\n",
      "  banking (6.68)\n",
      "\n",
      "Suggested themes per bank:\n",
      "BOA => ['cluster_0', 'cluster_2', 'cluster_1']\n",
      "CBE => ['cluster_0', 'cluster_2', 'cluster_1']\n",
      "Dashen => ['Transaction Performance', 'Account Access Issues']\n",
      "Saved task2_results.csv\n"
     ]
    }
   ],
   "source": [
    "# add src to path if needed\n",
    "import sys, os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "from sentiment import SentimentAnalyzer\n",
    "from themes import ThemeExtractor\n",
    "\n",
    "# 1) Load your clean dataset from Task 1\n",
    "df = pd.read_csv(\"clean_reviews.csv\")  # adjust path - if notebook inside notebook/ use \"../notebook/clean_reviews.csv\" or correct path\n",
    "\n",
    "# quick check\n",
    "print(df['bank'].value_counts())\n",
    "\n",
    "# 2) Sentiment analysis\n",
    "# Try transformer first (auto): will fall back to vader if transformers not available\n",
    "sa = SentimentAnalyzer(method=\"auto\", device=None)  # device=None => CPU\n",
    "sent_df = sa.analyze_series(df['review'], batch_size=64)\n",
    "\n",
    "# attach results\n",
    "df = pd.concat([df.reset_index(drop=True), sent_df.reset_index(drop=True)], axis=1)\n",
    "# save intermediate\n",
    "df.to_csv(\"task2_with_sentiment.csv\", index=False)\n",
    "print(\"Saved task2_with_sentiment.csv\")\n",
    "\n",
    "# Basic aggregation: mean sentiment_score per bank and rating\n",
    "agg = df.groupby(['bank','rating'])['sentiment_score'].agg(['count','mean'])\n",
    "print(agg.sort_values('count', ascending=False).head(20))\n",
    "\n",
    "# 3) Thematic analysis: extract TF-IDF keywords per bank\n",
    "te = ThemeExtractor(ngram_range=(1,2), top_k_keywords=30, min_df=2)\n",
    "bank_keywords = te.extract_bank_keywords(df, bank_col='bank', text_col='review')\n",
    "\n",
    "# Display top 15 keywords per bank\n",
    "for bank, kws in bank_keywords.items():\n",
    "    print(f\"\\nTop keywords for {bank}:\")\n",
    "    for w, score in kws[:15]:\n",
    "        print(f\"  {w} ({score:.2f})\")\n",
    "\n",
    "# 4) Map keywords to themes using a rule-based mapping (edit to taste)\n",
    "theme_mapping = {\n",
    "    \"Account Access Issues\": [\"login\", \"password\", \"fingerprint\", \"pin\", \"authenticate\", \"access\"],\n",
    "    \"Transaction Performance\": [\"slow\", \"delay\", \"transfer\", \"processing\", \"timeout\", \"speed\"],\n",
    "    \"UI & UX\": [\"ui\", \"interface\", \"layout\", \"design\", \"buttons\", \"navigation\"],\n",
    "    \"Crashes & Stability\": [\"crash\", \"freeze\", \"bug\", \"error\", \"exception\"],\n",
    "    \"Customer Support\": [\"support\", \"customer service\", \"help\", \"agent\", \"response\", \"call\"]\n",
    "}\n",
    "\n",
    "# Build theme suggestions per bank by mapping top keywords\n",
    "bank_themes = {}\n",
    "for bank, kws in bank_keywords.items():\n",
    "    keywords = [k for k,_ in kws]\n",
    "    matched_themes = te.rule_based_theme_mapping(keywords, theme_mapping)\n",
    "    # fallback: if fewer than 2 themes, run clustering on top keywords to suggest groups\n",
    "    if len(matched_themes) < 2:\n",
    "        clusters = te.cluster_keywords(keywords[:30], n_clusters=3)\n",
    "        # label clusters generically\n",
    "        matched_themes = [f\"cluster_{i}\" for i in clusters.keys()]\n",
    "    bank_themes[bank] = matched_themes\n",
    "\n",
    "print(\"\\nSuggested themes per bank:\")\n",
    "for b, t in bank_themes.items():\n",
    "    print(b, \"=>\", t)\n",
    "\n",
    "# 5) Assign themes to individual reviews via keyword matching (simple)\n",
    "def assign_themes_to_review(text, mapping):\n",
    "    text_l = text.lower()\n",
    "    themes = set()\n",
    "    for theme, kws in mapping.items():\n",
    "        for kw in kws:\n",
    "            if kw in text_l:\n",
    "                themes.add(theme)\n",
    "                break\n",
    "    return \";\".join(sorted(themes)) if themes else \"Other\"\n",
    "\n",
    "df['identified_themes'] = df['review'].astype(str).apply(lambda t: assign_themes_to_review(t, theme_mapping))\n",
    "\n",
    "# Save final CSV for Task 2\n",
    "out_cols = ['review','rating','date','bank','source','sentiment_label','sentiment_score','identified_themes']\n",
    "df[out_cols].to_csv(\"task2_results.csv\", index=False)\n",
    "print(\"Saved task2_results.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
